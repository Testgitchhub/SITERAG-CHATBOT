{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iQ1jnT0ULZ1cohYibQ_QFFWO9wbfyNu0",
      "authorship_tag": "ABX9TyPvl6cVPgkjZedrjNRY5oOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Testgitchhub/SITERAG-CHATBOT/blob/main/SITERAG_CHATBOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pyRjd39ouyM"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok flask flask-cors python-docx pdfplumber sentence-transformers faiss-cpu openai werkzeug==2.2.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_STORE = {} # doc_id -> { 'chunks': [text], 'meta': [ {start, end, filename}], 'emb_index': faiss_index }\n",
        "# Simple text extractor\n",
        "def extract_text_from_pdf(path_or_bytes):\n",
        "text = []\n",
        "if isinstance(path_or_bytes, (bytes, bytearray)):\n",
        "fp = io.BytesIO(path_or_bytes)\n",
        "with pdfplumber.open(fp) as pdf:\n",
        "for p in pdf.pages:\n",
        "text.append(p.extract_text() or '')\n",
        "else:\n",
        "with pdfplumber.open(path_or_bytes) as pdf:\n",
        "for p in pdf.pages:\n",
        "text.append(p.extract_text() or '')\n",
        "return \"\\n\".join(text)\n",
        "def extract_text_from_docx(path_or_bytes):\n",
        "if isinstance(path_or_bytes, (bytes, bytearray)):\n",
        "fp = io.BytesIO(path_or_bytes)\n",
        "doc = docx.Document(fp)\n",
        "else:\n",
        "doc = docx.Document(path_or_bytes)\n",
        "paragraphs = [p.text for p in doc.paragraphs]\n",
        "return \"\\n\".join(paragraphs)\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "words = text.split()\n",
        "chunks = []\n",
        "i = 0\n",
        "while i < len(words):\n",
        "chunk = words[i:i+chunk_size]\n",
        "chunks.append(' '.join(chunk))\n",
        "i += chunk_size - overlap\n",
        "return chunks\n",
        "def build_faiss_index(embs):\n",
        "index = faiss.IndexFlatL2(embs.shape[1])\n",
        "index.add(embs)\n",
        "return index\n",
        "def get_embeddings(texts):\n",
        "embs = embed_model.encode(texts, show_progress_bar=False, convert_to_numpy=True)\n",
        "return embs\n",
        "def call_llm_system(prompt, max_tokens=300, temperature=0.1):\n",
        "# Uses OpenAI ChatCompletion (gpt-3.5-turbo). Replace if you want other models.\n",
        "if not openai.api_key:\n",
        "raise ValueError('OPENAI_API_KEY not set in Colab env')\n",
        "res = openai.ChatCompletion.create(\n",
        "model='gpt-3.5-turbo',\n",
        "messages=[\n",
        "{\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based only on the provided context. If the answer is not in the context, say you don't know.\"},\n",
        "{\"role\": \"user\", \"content\": prompt},\n",
        "],\n",
        "temperature=temperature,\n",
        "max_tokens=max_tokens,\n",
        ")\n",
        "return res['choices'][0]['message']['content'].strip()"
      ],
      "metadata": {
        "id": "GuUNWcVFpC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "data = f.read()\n",
        "lower = filename.lower()\n",
        "if lower.endswith('.pdf'):\n",
        "text = extract_text_from_pdf(data)\n",
        "elif lower.endswith('.docx'):\n",
        "text = extract_text_from_docx(data)\n",
        "else:\n",
        "try:\n",
        "text = data.decode('utf-8')\n",
        "except Exception:\n",
        "return jsonify({'error':'unsupported file type'}), 400\n",
        "chunks = chunk_text(text, chunk_size=300, overlap=30)\n",
        "embs = get_embeddings(chunks)\n",
        "index = build_faiss_index(embs)\n",
        "doc_id = str(len(DOC_STORE) + 1)\n",
        "DOC_STORE[doc_id] = {\n",
        "'filename': filename,\n",
        "'chunks': chunks,\n",
        "'embs': embs,\n",
        "'index': index,\n",
        "}\n",
        "return jsonify({'doc_id': doc_id, 'n_chunks': len(chunks)})\n",
        "@app.route('/query', methods=['POST'])\n",
        "def query_doc():\n",
        "data = request.get_json(force=True)\n",
        "doc_id = data.get('doc_id')\n",
        "question = data.get('question')\n",
        "top_k = int(data.get('top_k', 4))\n",
        "if not doc_id or not question:\n",
        "return jsonify({'error':'doc_id and question required'}), 400\n",
        "if doc_id not in DOC_STORE:\n",
        "return jsonify({'error':'doc_id not found'}), 404\n",
        "store = DOC_STORE[doc_id]\n",
        "q_emb = get_embeddings([question])\n",
        "D, I = store['index'].search(q_emb, top_k)\n",
        "retrieved = [store['chunks'][int(i)] for i in I[0] if i != -1]\n",
        "# Build prompt\n",
        "context = \"\\n\\n---\\n\\n\".join(retrieved)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer concisely and cite the chunk if needed.\"\n",
        "try:\n",
        "answer = call_llm_system(prompt)\n",
        "except Exception as e:\n",
        "return jsonify({'error': str(e)}), 500\n",
        "return jsonify({'answer': answer, 'retrieved_chunks': retrieved})\n",
        "# Start Flask\n",
        "print('Starting Flask server with ngrok...')\n",
        "app.run()"
      ],
      "metadata": {
        "id": "3MyyC2jvpVuL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}